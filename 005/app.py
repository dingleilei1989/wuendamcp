import json
import logging
import os
import time
from typing import Dict, Any, Generator
import requests
from flask import Flask, request, jsonify, Response, stream_with_context

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('llm_proxy.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('LLMProxy')

class OpenAIClient:
    """OpenAIå®¢æˆ·ç«¯å°è£…"""

    def __init__(self, api_key: str, base_url: str = "https://api.openai.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })

    def make_request(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """å‘OpenAI APIå‘é€è¯·æ±‚"""
        url = f"{self.base_url}/{endpoint}"

        try:
            # è®°å½•å‘é€çš„è¯·æ±‚æ•°æ®
            logger.info(f"å‘é€è¯·æ±‚åˆ°: {url}")
            logger.info(f"è¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
            
            response = self.session.post(url, json=data, timeout=60)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAI APIè¯·æ±‚å¤±è´¥: {e}")
            logger.error(f"è¯·æ±‚URL: {url}")
            logger.error(f"è¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
            raise

    def stream_request(self, endpoint: str, data: Dict[str, Any]) -> Generator[bytes, None, None]:
        """å‘OpenAI APIå‘é€æµå¼è¯·æ±‚"""
        url = f"{self.base_url}/{endpoint}"
        
        try:
            # è®°å½•å‘é€çš„è¯·æ±‚æ•°æ®
            logger.info(f"å‘é€æµå¼è¯·æ±‚åˆ°: {url}")
            logger.info(f"è¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
            
            response = self.session.post(url, json=data, timeout=60, stream=True)
            response.raise_for_status()
            
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    yield chunk
        except requests.exceptions.RequestException as e:
            logger.error(f"OpenAI APIæµå¼è¯·æ±‚å¤±è´¥: {e}")
            logger.error(f"è¯·æ±‚URL: {url}")
            logger.error(f"è¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n".encode('utf-8')

class LLMProxy:
    """LLMä»£ç†æœåŠ¡"""

    def __init__(self, openai_api_key: str, openai_base_url: str = None):
        openai_base_url = openai_base_url or "https://api.openai.com/v1"
        self.openai_client = OpenAIClient(openai_api_key, openai_base_url)

    def process_chat_completion(self, request_data: Dict[str, Any]) -> Dict[str, Any] | Generator[bytes, None, None]:
        """å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚"""
        # æ‰“å°è¾“å…¥å‚æ•°
        self._log_request(request_data)

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æµå¼è¾“å‡º
            if request_data.get("stream", False):
                # æµå¼è¾“å‡º
                def generate() -> Generator[bytes, None, None]:
                    full_response = ""
                    try:
                        for chunk in self.openai_client.stream_request("chat/completions", request_data):
                            # ç´¯ç§¯å®Œæ•´å“åº”ç”¨äºæ—¥å¿—è®°å½•
                            if chunk.startswith(b"data: "):
                                try:
                                    data_str = chunk.decode('utf-8')[6:]  # ç§»é™¤ "data: " å‰ç¼€
                                    if data_str.strip() != "[DONE]":
                                        data = json.loads(data_str)
                                        if "choices" in data and len(data["choices"]) > 0:
                                            delta = data["choices"][0].get("delta", {})
                                            if "content" in delta:
                                                content = delta["content"]
                                                if content is not None:
                                                    full_response += content
                                                    
                                            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                                            if "tool_calls" in delta:
                                                self._log_tool_calls(delta["tool_calls"])
                                       
                                except (json.JSONDecodeError, KeyError):
                                    pass
                            yield chunk
                        
                        # è®¡ç®—è€—æ—¶
                        duration = time.time() - start_time
                        # æ‰“å°å®Œæ•´å“åº”
                        self._log_stream_response(full_response, duration)
                    except Exception as e:
                        logger.error(f"æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                        yield f"data: {json.dumps({'error': str(e)})}\n\n".encode('utf-8')
                
                return generate()
            else:
                # éæµå¼è¾“å‡º
                response = self.openai_client.make_request("chat/completions", request_data)
                # è®¡ç®—è€—æ—¶
                duration = time.time() - start_time
                # æ‰“å°å“åº”
                self._log_response(response, duration)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if "choices" in response and len(response["choices"]) > 0:
                    choice = response["choices"][0]
                    if "message" in choice and "tool_calls" in choice["message"]:
                        self._log_tool_calls(choice["message"]["tool_calls"])
                
                return response

        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "error": str(e),
                "object": "chat.completion",
                "created": int(time.time()),
                "model": request_data.get("model", "unknown")
            }

    def process_completion(self, request_data: Dict[str, Any]) -> Dict[str, Any] | Generator[bytes, None, None]:
        """å¤„ç†æ–‡æœ¬è¡¥å…¨è¯·æ±‚"""
        self._log_request(request_data)

        start_time = time.time()

        try:
            if request_data.get("stream", False):
                # æµå¼è¾“å‡º
                def generate() -> Generator[bytes, None, None]:
                    full_response = ""
                    try:
                        for chunk in self.openai_client.stream_request("completions", request_data):
                            # ç´¯ç§¯å®Œæ•´å“åº”ç”¨äºæ—¥å¿—è®°å½•
                            if chunk.startswith(b"data: "):
                                try:
                                    data_str = chunk.decode('utf-8')[6:]  # ç§»é™¤ "data: " å‰ç¼€
                                    if data_str.strip() != "[DONE]":
                                        data = json.loads(data_str)
                                        if "choices" in data and len(data["choices"]) > 0:
                                            text = data["choices"][0].get("text", "")
                                            if text is not None:
                                                full_response += text
                                except (json.JSONDecodeError, KeyError):
                                    pass
                            yield chunk
                        
                        # è®¡ç®—è€—æ—¶
                        duration = time.time() - start_time
                        # æ‰“å°å®Œæ•´å“åº”
                        self._log_stream_response(full_response, duration)
                    except Exception as e:
                        logger.error(f"æµå¼ä¼ è¾“è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                        yield f"data: {json.dumps({'error': str(e)})}\n\n".encode('utf-8')
                
                return generate()
            else:
                # éæµå¼è¾“å‡º
                response = self.openai_client.make_request("completions", request_data)
                duration = time.time() - start_time
                self._log_response(response, duration)
                return response

        except Exception as e:
            logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "error": str(e),
                "object": "text_completion",
                "created": int(time.time()),
                "model": request_data.get("model", "unknown")
            }

    def _log_request(self, request_data: Dict[str, Any]):
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        logger.info("=" * 80)
        logger.info("ğŸ“¥ æ”¶åˆ°LLMè¯·æ±‚")
        logger.info("=" * 80)

        # åŸºæœ¬ä¿¡æ¯
        model = request_data.get("model", "unknown")
        logger.info(f"ğŸ¤– æ¨¡å‹: {model}")

        # æ¶ˆæ¯å†…å®¹ï¼ˆèŠå¤©è¡¥å…¨ï¼‰
        if "messages" in request_data:
            messages = request_data.get("messages", [])
            logger.info("ğŸ’¬ æ¶ˆæ¯å†…å®¹:")
            for i, msg in enumerate(messages):
                role = msg.get("role", "unknown")
                content = msg.get("content", "")
                logger.info(f"  {i + 1}. [{role.upper()}] {content}")

        # æç¤ºè¯ï¼ˆæ–‡æœ¬è¡¥å…¨ï¼‰
        if "prompt" in request_data:
            prompt = request_data.get("prompt", "")
            if isinstance(prompt, str):
                logger.info(f"ğŸ“ æç¤ºè¯: {prompt}")
            else:
                logger.info(f"ğŸ“ æç¤ºè¯: {json.dumps(prompt, ensure_ascii=False)}")

        # å‚æ•°é…ç½®
        logger.info("âš™ï¸ å‚æ•°é…ç½®:")
        params = [
            ("temperature", request_data.get("temperature")),
            ("max_tokens", request_data.get("max_tokens")),
            ("top_p", request_data.get("top_p")),
            ("frequency_penalty", request_data.get("frequency_penalty")),
            ("presence_penalty", request_data.get("presence_penalty")),
            ("stream", request_data.get("stream", False))
        ]

        for param_name, param_value in params:
            if param_value is not None:
                logger.info(f"  {param_name}: {param_value}")

    def _log_response(self, response: Dict[str, Any], duration: float):
        """è®°å½•å“åº”æ—¥å¿—"""
        logger.info("=" * 80)
        logger.info("ğŸ“¤ è¿”å›LLMå“åº”")
        logger.info("=" * 80)

        logger.info(f"â±ï¸ å“åº”æ—¶é—´: {duration:.2f}ç§’")

        if "error" in response:
            logger.error(f"âŒ é”™è¯¯: {response['error']}")
            return

        # èŠå¤©è¡¥å…¨å“åº”
        if "choices" in response and response["choices"]:
            choice = response["choices"][0]

            if "message" in choice:
                # èŠå¤©æ ¼å¼
                message = choice.get("message", {})
                role = message.get("role", "assistant")
                content = message.get("content", "")
                logger.info(f"ğŸ’­ åŠ©æ‰‹å›å¤ [{role}]: {content}")

                # å¦‚æœæœ‰å‡½æ•°è°ƒç”¨
                if "function_call" in message:
                    func_call = message["function_call"]
                    logger.info(f"ğŸ”§ å‡½æ•°è°ƒç”¨: {func_call.get('name', 'unknown')}")
                    logger.info(f"ğŸ“‹ å‡½æ•°å‚æ•°: {func_call.get('arguments', '')}")

            elif "text" in choice:
                # æ–‡æœ¬è¡¥å…¨æ ¼å¼
                text = choice.get("text", "")
                logger.info(f"ğŸ“„ ç”Ÿæˆæ–‡æœ¬: {text}")

        # ä½¿ç”¨æƒ…å†µç»Ÿè®¡
        if "usage" in response:
            usage = response["usage"]
            logger.info("ğŸ“Š ä½¿ç”¨ç»Ÿè®¡:")
            logger.info(f"  æç¤ºtoken: {usage.get('prompt_tokens', 0)}")
            logger.info(f"  å®Œæˆtoken: {usage.get('completion_tokens', 0)}")
            logger.info(f"  æ€»token: {usage.get('total_tokens', 0)}")

    def _log_stream_response(self, full_response: str, duration: float):
        """è®°å½•æµå¼å“åº”æ—¥å¿—"""
        logger.info("=" * 80)
        logger.info("ğŸ“¤ è¿”å›LLMæµå¼å“åº”")
        logger.info("=" * 80)

        logger.info(f"â±ï¸ å“åº”æ—¶é—´: {duration:.2f}ç§’")
        logger.info(f"ğŸ’­ åŠ©æ‰‹å®Œæ•´å›å¤: {full_response}")
        
    def _log_tool_calls(self, tool_calls):
        """è®°å½•å·¥å…·è°ƒç”¨æ—¥å¿—"""
        logger.info("ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨:")
        for i, tool_call in enumerate(tool_calls):
            if "function" in tool_call:
                function = tool_call["function"]
                logger.info(f"  {i+1}. å·¥å…·åç§°: {function.get('name', 'unknown')}")
                logger.info(f"     å·¥å…·å‚æ•°: {function.get('arguments', '')}")
                if "id" in tool_call:
                    logger.info(f"     è°ƒç”¨ID: {tool_call['id']}")


# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

# åˆå§‹åŒ–ä»£ç†ï¼ˆä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼‰
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    logger.warning("æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨è™šæ‹Ÿå¯†é’¥")
    openai_api_key = "sk-dummy-key"

openai_base_url = os.getenv("OPENAI_BASE_URL")  # å¯é€‰çš„è‡ªå®šä¹‰åŸºç¡€URL

llm_proxy = LLMProxy(openai_api_key, openai_base_url)


@app.route('/v1/chat/completions', methods=['POST'])
@app.route('/chat/completions', methods=['POST'])
def chat_completions():
    """å¤„ç†èŠå¤©è¡¥å…¨ç«¯ç‚¹"""
    try:
        request_data = request.get_json()
        response_data = llm_proxy.process_chat_completion(request_data)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼å“åº”
        if isinstance(response_data, Generator):
            return Response(stream_with_context(response_data), content_type='text/event-stream')
        else:
            return jsonify(response_data)
    except Exception as e:
        logger.error(f"èŠå¤©è¡¥å…¨å¤„ç†å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/v1/completions', methods=['POST'])
def completions():
    """å¤„ç†æ–‡æœ¬è¡¥å…¨ç«¯ç‚¹"""
    try:
        request_data = request.get_json()
        response_data = llm_proxy.process_completion(request_data)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼å“åº”
        if isinstance(response_data, Generator):
            return Response(stream_with_context(response_data), content_type='text/event-stream')
        else:
            return jsonify(response_data)
    except Exception as e:
        logger.error(f"æ–‡æœ¬è¡¥å…¨å¤„ç†å¤±è´¥: {e}")
        return jsonify({"error": str(e)}), 500


@app.route('/v1/models', methods=['GET'])
def models():
    """è¿”å›æ¨¡å‹åˆ—è¡¨ï¼ˆæ¨¡æ‹ŸOpenAIæ ¼å¼ï¼‰"""
    # è¿™é‡Œå¯ä»¥è¿”å›æ¨¡æ‹Ÿçš„æ¨¡å‹åˆ—è¡¨ï¼Œæˆ–è€…ä»£ç†åˆ°çœŸå®API
    return jsonify({
        "object": "list",
        "data": [
            {
                "id": "gpt-3.5-turbo",
                "object": "model",
                "created": 1687882410,
                "owned_by": "openai"
            },
            {
                "id": "gpt-4",
                "object": "model",
                "created": 1687882411,
                "owned_by": "openai"
            }
        ]
    })


@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return jsonify({"status": "healthy", "service": "LLM Proxy"})


if __name__ == '__main__':
    # å¯åŠ¨æœåŠ¡
    host = os.getenv('PROXY_HOST', '0.0.0.0')
    port = int(os.getenv('PROXY_PORT', 8000))

    logger.info(f"ğŸš€ å¯åŠ¨LLMä»£ç†æœåŠ¡åœ¨ {host}:{port}")
    logger.info(f"ğŸ”‘ ä½¿ç”¨çš„OpenAIå¯†é’¥: {openai_api_key[:10]}...")

    app.run(host=host, port=port, debug=False)